{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roblox RL NPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Import models defined in models.py\n",
    "from models import DQN, Sameple_DQN\n",
    "from gym_env import GridWorldEnv, RBXEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "A fixed-size buffer that stores recently observed transitions. Essentially, short-term memory but for RL. Uses sample() to randomly select a batch of transitions to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    # Double-ended queue with limited capacity\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # Save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter | Description | \n",
    "|---|---|\n",
    "| BATCH_SIZE | the number of transitions sampled from the replay buffer.\n",
    "| GAMMA | the discount factor as mentioned in the previous section.\n",
    "| EPS_START | the starting value of epsilon.\n",
    "| EPS_END | the final value of epsilon.\n",
    "| EPS_DECAY | controls the rate of exponential decay of epsilon, higher means a slower decay.\n",
    "| TAU | the update rate of the target network.\n",
    "| LR | the learning rate of the ``AdamW`` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, batch_size = 128, gamma = 0.99, epsilon_start = 0.99, epsilon_end = 0.01, epsilon_decay = 1000, tau = 0.005, alpha = 1e-4):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Utilities\n",
    "        self.memory = ReplayMemory(batch_size * 1000)\n",
    "        self.steps = 0\n",
    "        self.episode_reward = defaultdict(int)\n",
    "        self.episode_length = defaultdict(int)\n",
    "        self.episodes_total = 0\n",
    "\n",
    "        # This is a continuous observation space (env.observation_space.n for Discrete)\n",
    "        # One neural network for optimal policy and the other for behavior/exploration\n",
    "        state_space = env.observation_space.shape\n",
    "        state_space = (state_space[2], state_space[0], state_space[1])\n",
    "        self.policy_net = DQN(state_space, env.action_space.n).to(device)\n",
    "        self.target_net = DQN(state_space, env.action_space.n).to(device)\n",
    "        # Apply policy_net parameters (weights and biases) to target_net\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # Adam - SGD either one works\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.alpha)\n",
    "\n",
    "    # Epsilon greedy action selection\n",
    "    def select_action(self, state):\n",
    "        # Epsilon Threshold: Early in training -> prioritize exploring | Later in training -> exploit more\n",
    "        eps_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                        math.exp(-1.0 * self.steps / self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "\n",
    "        # If random number (0, 1) less than eps_threshold then we explore, else we exploit\n",
    "        if random.random() < eps_threshold:\n",
    "            # Explore: we choose a random action\n",
    "            return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            # Exploit: we choose the best action at the state\n",
    "            with torch.inference_mode(): \n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "            \n",
    "    # Optimize agent's neural network\n",
    "    def optimize_model(self):\n",
    "        # Can't optimize model when replay memory is not filled\n",
    "        if (len(self.memory) < self.batch_size):\n",
    "            return\n",
    "        # Sample transitions from replay memory\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose batch: convert batch-array of transitions to transition of batch-arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Filter out all the next_state that are the endpoints of an episode\n",
    "        next_state_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "                                        device=device, \n",
    "                                        dtype=torch.bool)\n",
    "        \n",
    "        # Concatenates each of the transitions (state, action, reward) into their respective batches\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        next_state_batch = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        # Get actions from states along the action vector\n",
    "        action_value = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Q(s', a') for all next_states that are not None\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.inference_mode():\n",
    "            # Double DQN -> Q_t(s', argmax Q_p(s', a'))\n",
    "            next_state_values[next_state_mask] = self.target_net(next_state_batch).max(1).values #gather(1, self.policy_net(next_state_batch).argmax(1, keepdim=True)).squeeze(1)\n",
    "        \n",
    "        # If the next_state is None, then the Q(s', a') term evaluates to 0\n",
    "        expected_action_value = next_state_values * self.gamma + reward_batch\n",
    "\n",
    "        # Loss function for DQN -> L(s) = MSE\n",
    "        # We use the Huber loss here as it is less sensitive to outliers\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "        loss = loss_fn(action_value, expected_action_value.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Clear all gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping - prevents them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft updates\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1 - self.tau)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "    \n",
    "    # Train agent - for any actual improvements we need more than 128 episodes as one batch = 128\n",
    "    def train(self, episodes: int = 128):\n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots()\n",
    "        for ep in range(episodes):\n",
    "            state_raw, _ = env.reset()\n",
    "\n",
    "            while True:\n",
    "                state_tensor = torch.tensor(state_raw, dtype=torch.float32, device=device)\n",
    "                state_tensor = state_tensor.permute(2, 0 ,1).unsqueeze(0)\n",
    "                action_tensor = self.select_action(state_tensor)\n",
    "                action = action_tensor.item()\n",
    "\n",
    "                next_state_raw, reward, done, truncated, _ = env.step(action)\n",
    "                next_state_tensor = torch.tensor(next_state_raw, dtype=torch.float32, device=device) \n",
    "                next_state_tensor = next_state_tensor.permute(2, 0 ,1).unsqueeze(0)\n",
    "                reward_tensor = torch.tensor([reward], device=device)\n",
    "\n",
    "                self.memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor)\n",
    "\n",
    "                self.episode_reward[self.episodes_total] += reward\n",
    "                self.episode_length[self.episodes_total] += 1\n",
    "                self.steps += 1\n",
    "\n",
    "                self.optimize_model()\n",
    "\n",
    "                if done or truncated:\n",
    "                    break\n",
    "                \n",
    "                state_raw = next_state_raw\n",
    "\n",
    "            # Update the plot every 10 episodes once training starts\n",
    "            if (ep % 5 == 0):\n",
    "                ax.clear()  # Clears the old plot\n",
    "                ax.plot(list(self.episode_length.keys()), list(self.episode_length.values()), 'b-')  # Plots the updated data\n",
    "                ax.set_xlabel(\"Episodes\")\n",
    "                ax.set_ylabel(\"Episode Length\")\n",
    "                ax.set_title(\"Episode Length Over Time\")\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(fig)\n",
    "                \n",
    "                print(\"Current step: \", self.steps)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            self.episodes_total += 1\n",
    "\n",
    "        return self.episodes_total, self.episode_length, self.episode_reward\n",
    "    \n",
    "    def get_models(self):\n",
    "        return self.policy_net, self.target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
